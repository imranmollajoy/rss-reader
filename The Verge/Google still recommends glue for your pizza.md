You may remember we all had a fun little laugh at [Google’s AI search results](/2024/5/30/24168344/google-defends-ai-overviews-search-results) telling us to put [glue in our pizza](/2024/5/23/24162896/google-ai-overview-hallucinations-glue-in-pizza). Internet legend Katie Notopoulos [made and ate a glue pizza](https://www.businessinsider.com/google-ai-glue-pizza-i-tried-it-2024-5). A good time was had by all! Except, whoopsie, Google’s AI is training on our good time.

I will grant the query “how much glue to add to pizza” is an unusual one — but not that unusual given the recent uproar around glue pizza. [As spotted by Colin McMillen on Bluesky](https://bsky.app/profile/mcmillen.dev/post/3kuo5jgqcmf2i), if you ask Google how much glue to add to your pizza, the right answer — none! — does not appear. Instead, it cites our girl Katie suggesting you add an eighth of a cup. Whoops!

You may be wondering if this is a faked screenshot. I wondered that, too. But *The Verge* confirmed by running our own query:

Please note the snippet from us suggesting 1/8 of a cup of Elmer’s glue in the pizza sauce.

Jake Kastrenakes

Just phenomenal stuff here, folks. Every time someone like me reports on Google’s AI getting something wrong, we’re training the AI to be *wronger*.

Those of us of a, ahem, *certain age* will remember the phenomenon of “[Google bombing](https://en.wikipedia.org/wiki/Google_bombing);” the classic example was using the words “miserable failure” with a link to George W. Bush. Done frequently enough, the result was that a Google search for “miserable failure” returned, well, George W. Bush. Google figured out how to squish this fun game at some point in the late 2000s, but with its new AI results, hey, the game’s back on! I am just going to write “miserable failure” in the same sentence as George W. Bush once more for old times’ sake, and maybe in a day or two, you’ll get a great new AI search result, who knows!

This is not, by the way, a universal problem. [I asked Perplexity.AI](https://www.perplexity.ai/search/how-much-glue-kVIr9qwoSImJLk_LHpjOww) how much glue to put on pizza, and it told me, “I would strongly advise against putting any glue on pizza. Glue is not an edible ingredient and consuming it could be toxic and harmful to your health.” It then goes on to explain how the “glue on pizza” meme originated.

Perplexity, a buzzkill, suggests not putting glue on pizza.

Elizabeth Lopatto

ChatGPT doesn’t recommend glue on pizza, either:

ChatGPT is against glue on pizza, on the grounds that is possibly bad for you.

Elizabeth Lopatto

Naturally, this is not the only thing that is going wrong, though it is probably the funniest. This other thing is pretty good though: Google can’t answer questions about its own products anymore, thanks to its AI. Verge editor Richard Lawler [asked how to turn on screenshots](https://www.threads.net/@richardlawler/post/C7_9PWKpQg7) in Chrome’s Incognito mode. Google’s AI gave two answers, both wrong. In one, it suggests taking a screenshot in a normal Chrome tab.

Check out how the wrong AI overview bumps out the correct answer, making Google’s search less useful!

Richard Lawler

In the other, Google’s AI insists that taking a screenshot in Chrome’s Incognito mode simply isn’t possible:

Wrong again!

Richard Lawler

Unfortunately, by describing this problem, I am fairly sure I am now making it worse. Google is going to slurp up my fine prose describing the issue and feed it back to the unwary as proof that Chrome Incognito screenshots are impossible, and that glue belongs on your pizza. What will mischievous bloggers do with this information, I wonder?
